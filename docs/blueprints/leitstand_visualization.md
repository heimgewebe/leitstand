# Blaupause: Visualisierung des Heimgewebes im Leitstand

## Phase 1: Anatomie (Strukturelle Übersicht)

**Zielsetzung:** Darstellung der statischen Struktur des Heimgewebe-Organismus. In dieser Phase wird sichtbar, welche Organe (Repos/Services) existieren und wie sie konzeptionell miteinander verbunden sind. Ziel ist ein Gesamtüberblick über alle Komponenten und ihre Beziehungen, um das „Wer ist wer“ im System zu verstehen – das Organigramm des Heimgewebes. Dies schafft gemeinsame Referenz für alle weiteren Phasen (Wozu? → Orientierungswissen über den Systemaufbau).

**Eingesetzte Repos und ihre Rollen:** Im Heimgewebe wirken mehrere spezialisierte Repositories zusammen, die analog zu Organen verschiedene Aufgaben erfüllen. Relevante Repos für die Anatomie-Phase sind u.a.:

*   **metarepo – Governance & Contracts:** Enthält zentrale Architektur-Dokumente, Definition aller internen Organismus-Contracts (z.B. event.*, insights.daily, fleet.health) sowie Cross-Repo-Richtlinien. Hier werden neue Formate/Strukturen zuerst als Schema festgelegt (Prinzip “Contracts first”).
*   **chronik – Ereignis-Organ (Gedächtnis):** Hält den zentralen Ereignis-Store (event.line Append-only Log) und repräsentiert die Vergangenheit des Organismus.
*   **semantAH – Wissens-Organ:** Analysiert und rekonstruiert Wissen aus Events und anderen Quellen (Insights, Einbettungen, Graph-Strukturen) – die semantische Schicht.
*   **hausKI – Entscheidungszentrum:** Trifft als KI-Agent Entscheidungen auf Basis von Events, Insights und Metriken; steuert Aktionen (→ Kognitives Kernorgan).
*   **wgx – Fleet-Motorik:** Orchestriert als „Nervensystem“ die Fleet (alle Repos) in CI/CD: führt Standard-Checks aus (guard, smoke, etc.), generiert Metriken und Fleet-Zustandsbilder.
*   **leitstand – User-Interface:** Das Dashboard bzw. „Auge“ des Organismus zur Visualisierung von Events, Insights und Fleet-Status.
*   **heimgeist – Meta-Agent (Reflexion):** Beobachtet als “Geist des Heims” die Systemzustände (Events, Metriken, Insights) und erkennt Muster, Lücken, Drift zur Selbstreflexion des Systems.

(Weitere angrenzende Organe wie aussensensor für externe Sensor-Daten, heimlern für Mustererkennung, sichter für Code-Reviews, etc. sind Teil der Gesamtstruktur, werden hier aber der Kürze halber nur erwähnt. Alle zusammen bilden den verteilten, wissensbasierten Organismus Heimgewebe.)

**Artefakte und Datenflüsse:** Der strukturelle Snapshot der Anatomie wird als Graph aller Organe und ihrer Beziehungen festgehalten (geplant als Format wie anatomy.snapshot in JSON/Markdown). Grundlage sind definierte Contracts und Fleet-Konfigurationen im Metarepo (z.B. fleet/repos.yml mit der Liste aller Repos). Daraus generieren Tools ein Artefakt, das z.B. alle Knoten (Repos) und Kanten (Interaktionen/Abhängigkeiten) enthält. Dieser Snapshot ändert sich nur bei strukturellen Änderungen (neue Repos, geänderte Contracts o.Ä.) und fließt als statischer Input in den Leitstand. Datenflüsse gibt es in Phase 1 vor allem durch das Auslesen von Repository-Metadaten: Das Tooling (z.B. Lenskit im tools-Repo) sammelt alle relevanten Strukturdaten aus den Repos und dem Metarepo und fusioniert sie zu einem Gesamtbild. Es handelt sich also um einen statischen Export der Organismus-Struktur.

**Verantwortlichkeiten pro Repo:** Die Erstellung des Anatomie-Snapshots übernimmt primär das tools-Repo (insb. Merger/Lenskit), indem es Quellcode, Doku und Config aller Repos analysiert und die Gesamtstruktur extrahiert. Das metarepo liefert dabei die Definitionsgrundlage (Contracts, Verzeichnisse der Fleet) und validiert die Struktur konzeptionell (Architektur-Policies). Ein spezieller CI-Guard (z.B. organismus-readme-guard.yml) stellt sicher, dass Dokumentation der Organismus-Struktur aktuell bleibt – d.h. Änderungen in der Fleet-Struktur müssen im Snapshot/Doc reflektiert werden (sonst schlägt der Guard fehl). Der Leitstand konsumiert den Anatomy-Snapshot und bildet daraus eine Graph-Ansicht ab. Kurz: Tools/Metarepo produzieren, Leitstand konsumiert, CI-Guards prüfen Konsistenz.

**Visualformen im Leitstand:** Als Graph-Visualisierung – z.B. ein interaktives Netzdiagramm aller Organe. Jedes Repository wird als Knoten dargestellt (ggf. gruppiert nach Schichten/Achsen: z.B. Sensorik, Speicherung, Semantik, Entscheidung, Meta). Beziehungen oder Datenflüsse erscheinen als Kanten (gerichtet, wo zutreffend). Diese Graph-Ansicht zeigt die Topologie des Heimgewebes auf einen Blick. Zusätzlich könnte eine Matrix-Übersicht (Tabelle) die Repos vs. Achsen darstellen, ähnlich der Repo×Achsen-Matrix aus der Doku, um Produktions-/Konsum-Beziehungen je Achse zu verdeutlichen.

**Schema- oder Contract-Typen:** Für diese Phase ist ein eigenes Schema geplant, z.B. anatomy.snapshot (JSON-Schema für einen Struktur-Snapshot des Organismus). Dieses Schema würde definieren, wie ein Zustandsbild der System-Topologie strukturiert ist – enthalten z.B. alle Repos, ihre Rollen, Links zu Contracts und Interaktionen. Außerdem relevant: repos.yml (Fleet-Definition) im Metarepo als Contract der zweiten Ordnung für den Organismus. Weitere Contracts indirekt wichtig: interne Contracts wie event.line, fleet.health etc. werden im Anatomy-Snapshot referenziert, um anzuzeigen, welche Verbindungen über welche Schnittstellen laufen. (Beispiel: Kante Chronik→Leitstand via event.line.) Die Verbindlichkeit dieser Contracts garantiert, dass das Strukturmodell formal korrekt bleibt.

**Guards, Validierung und Update-Zyklen:** Die strukturelle Blaupause wird bei Änderungen am System erneuert. Typischerweise bei jedem Merge in zentrale Repos oder per täglichem CI-Run aktualisiert das tools-Merger-Skript den Anatomy-Snapshot. Ein Guard prüft, ob die aktuelle Struktur-Doku zu metarepo passt (z.B. ob alle aufgelisteten Repos tatsächlich existieren und alle existierenden Repos dokumentiert sind). Integrität wird durch Schema-Validierung (anatomy.snapshot muss dem Schema genügen) und durch Abgleich mit fleet/repos.yml sichergestellt. Der Update-Zyklus ist also ereignisgetrieben (bei Änderungen) und mindestens periodisch (z.B. tägliche Synchronisation), um Drift zwischen Dokumentation und Realität zu vermeiden.

**Ungewissheiten und epistemische Leerräume:** Trotz definierter Struktur können in Phase 1 implizite Abhängigkeiten oder emergente Kopplungen unsichtbar bleiben – etwa wenn zwei Komponenten indirekt über Dritte interagieren oder implizite Annahmen im Code bestehen, die nicht als formale Kante modelliert sind. Diese unbekannten Verbindungen sind bewusst (noch) unmessbar, da die Blaupause nur das erfasst, was als Contract/Repo-Relation dokumentiert wurde. Ebenso bleibt die Qualität oder Stärke einer Verbindung ungemessen: Die Anatomie zeigt dass Komponenten verbunden sind, aber nicht, wie kritisch diese Verbindung ist. Diese epistemischen Lücken in der Struktur werden in späteren Phasen (z.B. durch Events oder Metriken) weiter beleuchtet, bleiben aber auf reiner Strukturebene bestehen.

## Phase 2: Physiologie (Betriebszustand & „Vitaldaten“)

**Zielsetzung:** Visualisierung der dynamischen Zustände und Vitalzeichen des Heimgewebes. Phase 2 macht den “Gesundheitszustand” der einzelnen Organe und der Fleet als Ganzes sichtbar: Welche Dienste sind aktiv/gesund, wo gibt es Fehler oder Auffälligkeiten, wie ist die Performance? Es geht um das Funktionieren des Organismus im Betrieb – ähnlich wie Vitalparameter (Puls, Temperatur) bei einem Lebewesen. Damit erkennt man schnell, wo Handlungsbedarf besteht (Wozu? → Früherkennung von Fehlern, Monitoring der Systemgesundheit).

**Eingesetzte Repos und ihre Rollen:**

*   **wgx** steht im Zentrum: Als Fleet-Orchestrator misst und sammelt WGX laufend den Health-Status aller Repos/Services. Es startet z.B. regelmäßige Guard/Smoke-Checks und aggregiert Ergebnisse.
*   **metarepo** stellt das Schema für Gesundheitsdaten bereit (contracts/fleet.health.schema.json etc.) und definiert, welche Details erfasst werden (z.B. pro Repo Detailinfos).
*   **leitstand** fungiert als Konsument und Darsteller dieser Daten: Es bezieht die Fleet-Health-Snapshots und visualisiert sie als Übersichts-Dashboard oder Overlays.
*   **semantAH** kann die Roh-Metriken ebenfalls konsumieren, um sie mit den Wissensdaten zu verweben (z.B. Korrelation von Fehlerraten mit bestimmten Events).
*   **heimgeist** nutzt die Health-Daten zur Meta-Überwachung (Erkennen von abweichendem Verhalten oder dauerhaft degradierten Komponenten).

(Andere Repos: chronik spielt hier indirekt eine Rolle, indem es ggf. Metrik-Events aufzeichnet, z.B. könnte WGX Fehler als Events loggen. Services wie hausKI beziehen Metriken, um Entscheidungen zu beeinflussen, sind aber in der Visualisierung weniger direkt sichtbar.)

**Artefakte und Datenflüsse:** Zentrale Artefakte sind Snapshot-Dateien der Gesundheitszustände, v.a. fleet.health (aggregierter Fleet-Gesundheitszustand) und metrics.snapshot (detaillierte Metriken). Diese werden typischerweise bei jedem Lauf der CI-Checks oder mindestens täglich erzeugt. Datenfluss: WGX sammelt in jedem Durchlauf (z.B. bei nightly Builds oder Post-Merge) Metriken aus den Repos (Testergebnisse, Linter, Coverage, Performance-Messpunkte etc.) und schreibt einen Fleet-Health-Snapshot. Dieser Snapshot enthält Gesamtstatus plus Details pro Einheit (Repo/Service). Die Snapshots werden dann vom Leitstand eingelesen (z.B. via API oder Shared Storage) und als aktueller Zustand angezeigt. Zusätzlich können Metrik-Events fließen: WGX oder einzelne Checks können Events (nach Schema event.line) an chronik senden, etwa bei bestimmten Schwellenwert-Verletzungen (z.B. Build failed Event). Insgesamt fließen die Vitaldaten somit teils als strukturierte Snapshots, teils als Events ins System und werden im Leitstand konsolidiert dargestellt.

**Verantwortlichkeiten pro Repo:** Erzeugung: WGX ist der Producer der Fleet-Health-Daten – es führt in jedem Repo definierte Checks aus und konsolidiert deren Ergebnisse zu fleet.health. Die einzelnen Repos liefern die Rohdaten (Tests, Metrics) an WGX (über dessen Standard-Workflows). Validierung: Metarepo definiert das Schema, gegen das jeder fleet.health-Snapshot validiert wird; WGX hält sich an dieses Schema (bei Abweichungen würde ein CI-Test fehlschlagen). Es gibt z.B. automatisierte Tests (metrics_snapshot.bats) in wgx, die prüfen, ob generierte Snapshots valide sind. Konsum: Leitstand liest die neuesten Snapshots und aktualisiert die UI. semantAH kann zusätzlich als Konsument agieren, um Metriken in längeren Zeitreihen zu analysieren oder anzureichern. Heimgeist und ggf. hausKI beobachten die Metriken ebenfalls (Heimgeist für Anomalieerkennung, hausKI für Entscheidungsfindung).

**Visualformen im Leitstand:** Ein Fleet-Status-Dashboard mit Ampelübersicht pro Komponente (grün/gelb/rot für gesund/warnend/fehlerhaft). Dies kann als Layer über dem Struktur-Graphen von Phase 1 gelegt werden – z.B. durch farbige Markierung der Knoten entsprechend ihrem Status (Anatomie + Vitaldaten kombiniert). Zusätzlich sind Detail-Panels je Organ möglich (bei Klick): z.B. Anzeige letzter Check-Zeit, Metriken (CPU, Memory, Durchsatz), Anzahl Errors, etc. Aggregate (z.B. „Fleet insgesamt: 95% grün“) können in Kopfzeile oder separatem Health-Panel visualisiert werden. Wichtig ist die Echtzeit- bzw. Near-Realtime-Aktualisierung: Änderungen (neue Runs, Fehler) sollten zeitnah im Leitstand auftauchen. Mögliche Visualformen: Tacho/Trend-Anzeigen für Performance-Metriken, Heatmaps über Zeit, oder ein Status-Stream (Ticker) für neu auftretende Warnungen.

**Schema- oder Contract-Typen:** Fleet Health Snapshot (fleet.health): JSON-Schema, das den aggregierten Gesundheitszustand definiert. Darin: ein Timestamp, ein Summary-Status und eine Liste details[] pro Einheit mit Name, Status, optional Message/Codes. Metrics Snapshot (metrics.snapshot): Schema für punktuelle Metrik-Messungen (z.B. spezielle Kennzahlen aus Performance-Tests). Metrik-Events: können als event.metric (Unterform von event.line) definiert sein – standardisierte Events, die bestimmte Schwellenüber- oder -unterschreitungen signalisieren. Diese Contracts liegen alle im Metarepo (Organismus-intern) und werden von der gesamten Fleet eingehalten. Somit ist sichergestellt, dass z.B. WGX und Leitstand ein gemeinsames Verständnis der „Health“-Daten haben.

**Guards, Validierung und Update-Zyklen:** Guards: In jedem Repo sorgt der WGX-Guard Workflow dafür, dass Checks laufen; falls z.B. Tests fehlschlagen, wird das im fleet.health vermerkt. Ein WGX-Metrics-Snapshot-Script läuft regelmäßig (mindestens 1× täglich oder bei bestimmten Events wie Merge) und erzeugt aktualisierte Snapshots. Validierungen passieren auf zwei Ebenen: (1) Schema-Validierung – das generierte JSON muss dem Schema entsprechen (WGX-Skripte und Tests checken das), (2) Integritätsregeln – z.B. dass alle Fleet-Repos im Snapshot vertreten sind, keine fehlen. Update-Zyklus: kontinuierlich und zeitgesteuert. Kontinuierlich, weil bei jedem CI-Durchlauf neue Daten entstehen; zeitgesteuert, indem mindestens einmal pro Tag (z.B. jede Nacht bis 08:00 Uhr) ein Snapshot publiziert wird. Der Leitstand pollt oder empfängt diese Updates und aktualisiert die Anzeige automatisch. So bleibt der angezeigte Zustand immer frisch.

**Ungewissheiten und epistemische Leerräume:** Die Systemgesundheit ist vielschichtig und nicht vollständig messbar. Einige Aspekte bleiben bewusst ungemessen: z.B. „latente Probleme“, die (noch) keine Metrik verletzt haben, oder Qualitätsaspekte wie Code-Verständlichkeit werden von fleet.health nicht erfasst. Auch können grüne Checks eine falsche Sicherheit vermitteln – unbekannte Fehlerpfade bleiben verborgen, solange keine Metrik dafür existiert. Zudem: Ist ein Dienst ausgefallen und der Mechanismus, der das messen würde, ebenfalls ausgefallen, bleibt dies eine blinde Stelle. Solche epistemischen Lücken (z.B. fehlende Sensoren für bestimmte Fehlerarten) sind per Definition schwer messbar. Das Heimgewebe adressiert dies in späteren Phasen durch Meta-Analyse (Phase 5), kann aber nie alle unbekannten Unbekannten erfassen. Wichtig ist, dass bewusst keine willkürliche „Gesamtnote“ vergeben wird – die Vitaldaten bleiben granular, um nicht Scheingenauigkeit vorzutäuschen. Ungewissheit besteht also in dem, was außerhalb der definierten Metriken liegt.

## Phase 3: Zeitachse (Temporale Ereignisse)

**Zielsetzung:** Darstellung der Historie und chronologischen Abfolge aller relevanten Ereignisse im Heimgewebe. Phase 3 macht transparent, was im System wann passiert ist – von externen Umweltereignissen bis internen Zustandsänderungen – als lückenlose Timeline. Dadurch wird Ursache-Wirkung nachvollziehbar: Wann traten welche Fehler auf? Welche Sequenz führte zu einer Entscheidung? Ziel ist es, Zusammenhänge in der Zeit zu erkennen (Wozu? → Ursachenanalyse, Ablaufverfolgung, historisches Monitoring).

**Eingesetzte Repos und ihre Rollen:**

*   **chronik** ist der Herzschrittmacher der Zeitachse – ein spezialisiertes Repo/Service, das alle Events in append-only Manier aufzeichnet. Es fungiert als zentraler Event-Store und Zeitarchiv.
*   **Event-Produzenten:** Viele Repos schreiben Events: z.B. aussensensor sendet Außenwelt-Ereignisse (aussen.event.*), hausKI loggt Entscheidungen als Events, heimlern könnte Mustererkennungsergebnisse als Events zurückspielen, mitschreiber erzeugt OS-Kontext-Events (os.context.*), WGX generiert CI/Fleet-bezogene Events (z.B. Start/Ende von Läufen, Anomalien).
*   **Event-Konsumenten:** semantAH liest die Chronik, um daraus Wissen zu generieren (die Chronik ist Datenbasis für Analysen). leitstand konsumiert Events, um sie in der Timeline anzuzeigen. Auch heimgeist und heimlern abonnieren bestimmte Events, um darauf zu reagieren (Reflexion/Lernen).
*   **plexer** als Event-Router vermittelt Events in Echtzeit zwischen Komponenten: Es kann z.B. eingehende Events an mehrere Konsumenten verteilen (z.B. direkt an heimgeist), um schnelleres Routing als über Chronik zu ermöglichen. In der Visualisierung spielt Plexer weniger eine direkte Rolle, außer evtl. bei Live-Ansichten.
*   **metarepo** definiert wieder die Schema-Grundlage: das zentrale event.line Contract (JSONL-Format für Event-Einträge) liegt im Metarepo und gilt für alle.

**Artefakte und Datenflüsse:** Kern-Artefakt ist der Event-Stream selbst, typischerweise als fortlaufende Datei (event.line), in der jedes Event als JSON-Line appended wird. Datenfluss: Jeder Event-Produzent (Service oder Agent) sendet sein Event an chronik (über eine API oder Message, z.B. HTTP-POST), wo es persistent angehängt wird. Chronik ordnet Events zeitlich (ggf. mit Timestamp) und versieht sie mit Sequenznummer. Der Leitstand fragt diesen Event-Store ab (z.B. über einen chronik-API-Endpoint oder Streaming-Schnittstelle) und erhält neue Events kontinuierlich. So fließt ein kontinuierlicher Datenstrom ins UI (nahe Echtzeit). Zusätzlich: Chronik kann nachträgliche Abfragen (Queries) nach Zeitintervall oder Filter ermöglichen, damit der Leitstand historische Ausschnitte laden kann. Neben dem Hauptstrom gibt es evtl. Indices oder Snapshots: z.B. ein Tageszusammenfassung-Event oder Checkpoints, um lange Histories effizienter darzustellen. Aber primär bleibt das Event-Line-Log die Quelle. Wichtig: Jedes Event entspricht einem bestimmten Schema-Typ (Untertypen von event.line je nach Quelle), was in den Daten mitgespeichert ist (z.B. event type = aus.sensor.weather).

**Verantwortlichkeiten pro Repo:** Erzeugen: Alle Producer-Repos sind dafür verantwortlich, ihre Ereignisse im richtigen Format an Chronik zu liefern. Beispielsweise sendet aussensensor bei neuen Sensordaten ein Event, hausKI bei getroffener Entscheidung, WGX bei CI-Statuswechsel etc. Persistieren: chronik übernimmt als zentrales Organ die Verantwortung, Events dauerhaft, unveränderlich und in Ankunftsreihenfolge zu speichern. Es stellt auch sicher, dass jeder Event-Eintrag das Schema event.line erfüllt (nicht konforme Events lehnt es ab oder markiert sie). Validierung: Das Metarepo stellt das Schema bereit und meist sind in den Producer-Repos schon Contracts eingebunden, sodass Events korrekt formatiert erzeugt werden. Zusätzlich kann chronik als Gatekeeper wirken: z.B. laufend einen Integritätsreport generieren, der Schema-Abweichungen auflistet (in der Doku wird erwähnt, dass es einen Integrity-Check gibt, der Events ohne Provenienz in Phase 1 nur warnt). Konsumieren: Leitstand fragt Chronik fortlaufend ab – die Implementierung verantwortet leitstand (z.B. WebSocket für neue Events). semantAH und heimgeist haben eigene Ingest-Worker, die neue Events aus Chronik pollen oder via plexer erhalten.

**Visualformen im Leitstand:** Eine Timeline-Ansicht steht im Mittelpunkt. Konkret: eine scrollbare, zeitlich sortierte Liste aller Events mit Timestamp und kurzem Beschreibungstext. Farb- oder Icon-Codierung kann die Herkunft anzeigen (z.B. Sensor-Event, Decision-Event, etc.). Filter- und Zoom-Funktionen ermöglichen es, bestimmte Zeitfenster oder Event-Typen auszuwählen (etwa nur Fehler-Events der letzten 24h). Neben der reinen Liste sind Verknüpfungen wichtig: Klickt man z.B. auf ein Decision-Event, könnte rechts ein Panel aufklappen mit den zugehörigen Details (z.B. welche Insight und welcher Health-Status gingen voraus). Evtl. können zusammengehörige Events (Request/Response oder Ursache/Wirkung) verbunden dargestellt werden, etwa durch Einrücken oder Linien. Zusätzlich hilfreich: Timeline-Graph-Kombination – man könnte den Graph aus Phase 1 zeitlich animieren, z.B. indem Kanten aufblinken, wenn zwischen zwei Knoten ein Event fließt (Visualisierung der Live-Interaktionen). In jedem Fall vermittelt Phase 3 dem Benutzer eine Chronik der Fakten im System.

**Schema- oder Contract-Typen:** Event Stream (event.line): Das wichtigste Schema, definiert im Metarepo, Format meist JSON Lines. Es legt fest, welche Felder ein Event haben muss (Zeitstempel, Typ, Quelle, Payload etc.). Dazu kommen Sub-Schemas für verschiedene Event-Typen (z.B. aussen.event.weather, heimgewebe.command.v1, os.context.window usw.), damit die Payload jeweils validierbar ist. Published Events Doku: Es gibt im Metarepo oft eine Events-Übersicht (Liste aller Event-Typen und Bedeutung) – als Contract in Dokuform. Provenance/Origin Felder: sind Teil des Schemas (wer hat das Event erzeugt, in welchem Kontext). Weitere relevante Contracts: falls Chronik ein API hat, gibt es ggf. ein OpenAPI/Protobuf Schema dafür (aber das ist Laufzeit-Schnittstelle, nicht Wissenscontract). Wichtig: Audit-Contract – die Garantie, dass Events unveränderlich und nur anhängbar sind, ist eher Policy als Schema, aber essenziell. Insgesamt sorgen diese Contracts dafür, dass die Timeline normiert ist und Events aus verschiedenen Quellen wie Puzzleteile zusammenpassen.

**Guards, Validierung und Update-Zyklen:** Die Chronik selbst fungiert als laufender Guard: sie nimmt nur valide Events an (Schema-check pro Event). In frühen Phasen ist Provenienz (Herkunft) eventuell optional, aber später streng eingefordert (Konfig-Schalter CHRONIK_ENFORCE_PROVENANCE wird z.B. in Phase 2 aktiviert). Zusätzlich könnten Guards existieren, die prüfen, ob alle erwarteten Eventströme fließen – z.B. ein Watchdog, ob mitschreiber regelmäßig OS-Events liefert. Update-Zyklus: Hier kontinuierlich/ereignisgetrieben – jedes neue Event wird sofort persistiert und sollte beinahe in Echtzeit im Leitstand erscheinen. Die Timeline wird also laufend erweitert. Für Langzeitstabilität könnten nächtliche Jobs alte Events archivieren oder Indizes aktualisieren, aber aus Sicht der Visualisierung geschieht das transparent im Hintergrund. Integritätssicherung umfasst auch Backups (Events dürfen nicht verloren gehen) und ggf. Hash-Chains, um Manipulation auszuschließen (Audit-Trail).

**Ungewissheiten und epistemische Leerräume:** Ein Ereignis-Log zeigt nur, was als Event deklariert wurde. Ungemeldete Zustandsänderungen – also stille Side-Effects, die entgegen der Policy nicht als Event erfasst wurden – bleiben der Timeline verborgen. Das Heimgewebe-Design verlangt zwar „Events statt Seiteneffekte“, doch in der Praxis kann es Lücken geben (z.B. ein Entwickler vergisst, ein Event zu emitten). Solche blinden Flecken sind bewusst nicht messbar, da man ein unbekanntes Ereignis nicht loggen kann. Zudem kann die Bedeutung mancher Events unklar bleiben – der Log gibt das Faktum „X ist passiert“ wieder, aber das Warum oder folgende Implikationen sind epistemische Fragen für höhere Schichten. Auch gibt es potentiell eine Ereignisflut: Nicht alles kann angezeigt werden (Signal-to-Noise). Welche Events wirklich relevant sind, ist subjektiv – die Timeline selbst misst das nicht. Bewusst wird deshalb nichts „zusammengefasst“ in Phase 3 (kein Verlust an Granularität), was aber heißt, dass Bedeutsamkeit eines Events eine Leerstelle bleibt, die erst durch Interpretation (Phase 4/5) gefüllt wird.

## Phase 4: Erkenntnisschichten (Wissens- und Semantikebene)

**Zielsetzung:** Visualisierung der gelernten Erkenntnisse und semantischen Strukturen, die das Heimgewebe aus den Rohdaten gewinnt. Phase 4 hebt die Darstellung auf eine Bedeutungsebene: Was bedeuten die Daten? Welche Muster, Zusammenhänge oder Insights wurden extrahiert? Ziel ist es, dem Betrachter und dem System selbst die verdichteten Informationen zugänglich zu machen – z.B. in Form von Wissensgraphen, Zusammenfassungen oder Anomalie-Markierungen (Wozu? → Verständnistiefe erhöhen, Kontext liefern, versteckte Muster sichtbar machen).

**Eingesetzte Repos und ihre Rollen:**

*   **semantAH** ist das Zentrum der Wissensbildung: Dieser Dienst konsolidiert Events (Chronik), Kontext (OS via mitschreiber), ggf. Textdaten (vault-gewebe) und erzeugt höhere Erkenntnisse. Rollen: Datenintegrator und Analyst, zuständig für Embeddings, Clustering, semantische Graphen und insbesondere den täglich aggregierten Insight-Output (insights.daily).
*   **heimgeist** nutzt die Erkenntnisse aus semantAH, um auf Meta-Ebene zu bewerten (dazu Phase 5). In Phase 4 könnte heimgeist aber bereits als Konsument tiefer Erkenntnisse auftreten, z.B. um Lücken zu identifizieren.
*   **metarepo** stellt auch hier Contracts bereit: z.B. Schema für insights.daily und zugehörige Meta-Informationen. Außerdem evtl. für semantische Graph-Formate oder Embeddings (os.context.text.embed.schema.json wurde im Code erwähnt).
*   **lenskit** (im tools-Repo) spielt eine Hilfsrolle, indem es mehrstufige Repräsentationen der Wissensbasis erzeugt. Z.B. generiert es Snapshots in unterschiedlicher Detailtiefe (“dev”, “max”) und extrahiert Relationen, sodass KI-Agenten (oder auch Leitstand) darauf zugreifen können. Man kann es als Werkzeugkasten für Erkenntnisvisualisierung sehen.
*   **leitstand** greift die von semantAH gelieferten Insights auf und präsentiert sie in geeigneter Form (Graph-Ansicht der Knowledge-Graph, Layer-Ansichten mit Annotations, etc.).

(Nebenrollen: heimlern könnte hier relevant sein, indem erkannte Muster/Trends aus heimlern ebenfalls als Wissen einfließen. vault-gewebe (Obsidian Vault) liefert statisches Wissen, das semantAH in den Wissensgraph einbindet. Diese externen Wissensquellen erweitern die Erkenntnisschicht.)

**Artefakte und Datenflüsse:** Haupt-Artefakt ist insights.daily – eine täglich generierte Sammlung von Erkenntnissen/Auswertungen. Darin stehen z.B. aggregierte Statistiken, neue Korrelationen oder natürlichsprachliche Zusammenfassungen der letzten 24h. Daneben gibt es embeddete Wissensgraphen: semantAH baut z.B. einen Graph mit Entities und Relationen aus den Events (wer hat was wann) oder kontextualisiert OS-Information (z.B. „Dokument X bearbeitet während Event Y“). Dieser Wissensgraph kann als JSON-LD oder proprietäres Format vorliegen. Embedding-Vektoren: Für Texte oder Ereignisse berechnet semantAH Vektoren (zur semantischen Ähnlichkeitsberechnung), die als Artefakt (*.embed) vorliegen. Datenflüsse: semantAH zieht jede Nacht die neuen Events aus Chronik, die neuesten Fleet-Health-Daten und evtl. frische Vault-Notizen. Es verarbeitet diese (NLP, statistische Auswertung, Graph-Updates) und schreibt dann insights.daily (und aktualisiert ggf. einen persistenten Wissensgraph-Speicher). Der Leitstand holt insights.daily (z.B. als JSON vom semantAH-Output) am Morgen ab und zeigt die Inhalte an. Falls semantAH kontinuierliche Auswertungen macht (z.B. Online-Lernen), könnten auch Streaming-Insights fließen – etwa ein Event „Anomalie erkannt: Device offline seit 1h“ als Event oder als sofortiges Insight-Update. Größtenteils ist es aber batch-orientiert (über Nacht verdichtet).

**Verantwortlichkeiten pro Repo:** Produktion: semantAH ist klarer Producer der Insight-Daten. Es führt die Datenpipeline von Input (Chronik, Kontext) zu Output (Insights) aus. Dabei hält es sich an die definierten Knowledge-Contracts, z.B. schreibt es insights.daily exakt im Schema ab. Validierung: Diese erfolgt sowohl formal (Schema-Check des insights.daily Outputs, analog zu Tests wie bei fleet.health) als auch inhaltlich – semantAH hat möglicherweise Qualitätskriterien (z.B. Konsistenzprüfungen: Summe der Teilstatistiken = Gesamt etc.). Gegebenenfalls gibt es einen Review-Schritt (vielleicht durch heimgeist oder einen Entwickler), der neue Erkenntnisse plausibilisiert. Konsum: leitstand ist Hauptkonsument, um die Erkenntnisse anzuzeigen. heimgeist konsumiert die Insights ebenfalls, um Meta-Überlegungen anzustellen (z.B. ob wichtige Bereiche unbeobachtet blieben). hausKI könnte einige Insights (z.B. neu erkannte Korrelationen) für bessere Entscheidungen einfließen lassen – wobei hausKI primär Phase 3-Daten nutzt. Tools/Lenskit hilft bei Bereitstellung: es kann die Insight-Daten in verschiedene Formate konvertieren oder visualisierbare Artefakte erstellen (z.B. generiert Diagramme aus dem Knowledge-Graph, die Leitstand direkt einbetten kann).

**Visualformen im Leitstand:** Hier kommen Graph- und Layer-Visualisierungen zum Tragen. Eine mögliche Darstellung ist ein Wissensgraph: Knoten könnten z.B. Konzepte oder Entities sein (Personen, Geräte, Räume, Themen), Kanten deren Beziehungen (hat genutzt, verursacht, gehört zu etc.), generiert aus dem Kontext der Events. Dieser Graph erlaubt semantisches Browsen: der Nutzer sieht z.B. dass Gerät A und Gerät B ungewöhnlich oft zusammen ausfallen (Verbindung hervorgehoben). Eine andere Visualform sind Layer: Man kann über die Timeline oder den Struktur-Graph Erkenntnis-Layer legen – z.B. Highlights für abnormale Muster oder Cluster. Konkret: In der Timeline könnten bestimmte Zeitabschnitte farblich hinterlegt werden, weil semantAH ein Muster erkannte („Phase des hohen Fehlers“). Im Struktur-Graph könnten Layer-Overlays anzeigen, welche Verbindungen semantisch stark sind (dickere Linie, wenn häufig gemeinsame Events). Auch Textuelle Zusammenfassungen sind zentral: z.B. ein Dashboard-Widget „Tageszusammenfassung“ in natürlichsprachigem Text („Gestern haben sich zwei neue Fehlerklassen ergeben, die Auslastung war 20% höher als normal…“). Tabellen oder Charts können Trends zeigen (Beispiel: ein Chart mit der Anzahl Events pro Kategorie im Zeitverlauf, mit Annotationen aus Insights). Insgesamt geht es darum, rohe Daten in Wissen zu verwandeln und visuell greifbar zu machen – Graphen für Beziehungen, Layer zum Ein-/Ausblenden von Bedeutungsdimensionen, und erklärende Texte/Charts für Muster.

**Schema- oder Contract-Typen:** Insights-Schema (insights.daily.schema.json): definiert Aufbau der täglichen Erkenntnissammlung. Enthält vermutlich Felder wie Datum, Liste von Insight-Items (jede mit Typ, Beschreibung, ggf. Metrik/Statistik). Insights Meta (insights.daily.meta.json): kann ergänzende Metadaten beschreiben (z.B. Version, verwendete Methoden). Heimgeist Insight (heimgeist.insight.v1): mögliches Schema für spezielle Meta-Insights oder tiefergehende Erkenntnisse, die heimgeist als Agent formuliert – in den Files referenziert. Knowledge Graph Schema: ggf. ein internes JSON-LD oder GraphML Schema, um den semantischen Netzwerk-Zustand festzuhalten (nicht unbedingt als Contract in Metarepo, könnte innerhalb semantAH bleiben). Embedding/ML Model Contracts: falls semantAH Modelle trainiert (z.B. Clustering-Modelle), gibt es evtl. “model cards” oder zumindest Versionierungs-Contracts, jedoch weniger relevant für Visualisierung. Wichtig ist: Die Insights-Outputs sind über definierte Schemas standardisiert, sodass Leitstand und auch eventuelle externe Systeme (z.B. ein Knowledge Observatory) sie verarbeiten können. Die Trennung von Fakten und Erkenntnissen wird vertraglich gewahrt – Insights sind stets als abgeleitete Info gekennzeichnet, während Events rohe Fakten bleiben.

**Guards, Validierung und Update-Zyklen:** Guardrails: Der Prozess, der Insights generiert (z.B. ein täglicher semantAH-Batch-Job), hat eigene Fehlerüberwachung. Es wird geprüft, ob der Job erfolgreich durchlief und ob insights.daily erzeugt wurde bis zur gesetzten Zeit (z.B. 08:00). Falls nicht, kann ein Alert ausgelöst werden. SemantAH kann auch Validierungen einbauen, z.B. ob der neu berechnete Wissensgraph konsistent ist (keine verlorenen Kanten, keine abrupten Sprünge in Statistiken ohne Events etc.). Auf inhaltlicher Ebene mag es Guard-Kriterien geben: z.B. “kein Insight-Item soll komplett leer sein” oder “Jede Erkenntnis muss auf mindestens einem Event basieren” – um sinnvolle Outputs zu garantieren. Update-Zyklus: in der Regel täglich (daher daily Insights). Das heißt jede Nacht werden die neuen Daten der letzten 24h integriert. Zusätzlich können bei großen Änderungen Ad-hoc-Updates erfolgen (z.B. wenn ein seltener kritischer Vorfall passiert, könnte semantAH sofort ein Insight generieren, statt auf Nacht zu warten – je nach Implementierung). Die Visualisierung im Leitstand sollte sich synchronisieren, z.B. morgens automatisch die neue Insights-Datei laden. Inkrementelle Updates (tagsüber) könnten als Insight-Events fließen, aber das wäre schon Übergang zu Phase 5 (Meta/Reflexion, z.B. heimgeist meldet etwas). Integrity wird hier auch durch fortlaufende Evaluation gewahrt: Da semantAH komplexe Analysen fährt, wird man Ergebnisse rückvalidieren (vielleicht manuell durch Entwickler oder Vergleich mit Vortagen), um zu sehen, ob die Erkenntnisse plausibel sind – dies ist weniger formal, aber notwendig, um Fehllernen zu erkennen.

**Ungewissheiten und epistemische Leerräume:** In der Wissensschicht liegt eine große Quelle von Ungewissheit: Werthaltigkeit und Vollständigkeit der Erkenntnisse. Semantische Analysen können Irrtümer enthalten (z.B. Korrelation als Kausalität fehlzuinterpretieren). Solche Fehler sind nicht immer offensichtlich messbar. Zudem gibt es immer Unwissen im System: semantAH kann nur Erkenntnisse aus den verfügbaren Daten ziehen – fehlen Daten oder sind verzerrt, bleiben gewisse Schlüsse unerreichbar. Bewusst unmessbar sind z.B. Bedeutungen außerhalb des Modells: Das System weiß, was es modelliert hat, aber nicht, was es nicht modelliert hat. Beispiel: semantAH könnte nicht erkennen „Stimmung der Nutzer“ wenn dazu keine Datenquelle existiert – diese Leerstelle bleibt epistemisch vorhanden. Auch Mehrdeutigkeiten in Daten (z.B. ein Event könnte versch. Ursachen haben) führen zu Unsicherheit: semantAH liefert evtl. einen educated guess, aber keine absolute Gewissheit. Das wird im Leitstand idealerweise sichtbar gemacht – etwa durch Confidence-Werte oder Hinweis auf alternative Interpretationen, aber eine Rest-Unsicherheit bleibt. Epistemisch heikel ist auch die Grenze zwischen Fakt und Interpretation: Phase 4 darf nicht als absolut angesehen werden, sondern als Projektor der wahrscheinlichsten Bedeutung. Bewusst wird hier nichts „hart verdrahtet“ – die Entwickler lassen dem System Freiheiten, nicht alles quantifizieren zu müssen, um emergentes Verhalten nicht abzuwürgen (z.B. werden komplexe semantische Relationen nicht mit einer einfachen Kennzahl bewertet, weil das die Nuance killt). Kurz: Die Erkenntnisschicht bringt neues Wissen hervor, doch was das System nicht gelernt hat oder (noch) nicht versteht, bleibt ein blinder Fleck, den es selbst nicht direkt beleuchten kann.

## Phase 5: Reflexion (Meta-Analyse & Systemautopoiese)

**Zielsetzung:** Darstellung der selbstreflektiven Erkenntnisse des Heimgewebes – also wie das System über seinen eigenen Zustand und sein Wissen nachdenkt. In Phase 5 geht es um die Meta-Ebene: Wo liegen Drift, Lücken oder Optimierungspotenzial? Welche Veränderungen treten über die Zeit auf (Trends, Deltas)? Hier wird der Leitstand zu einem „epistemischen Projektor“ des Systems: Er zeigt, was das Heimgewebe über sich selbst gelernt hat. Ziel ist, Bewusstsein über Unbekanntes zu schaffen (Wozu? → kontinuierliche Verbesserung, Vertrauen durch Transparenz der Selbstüberwachung).

**Eingesetzte Repos und ihre Rollen:**

*   **heimgeist** ist der Hauptakteur dieser Phase. Als Meta-Agent beobachtet er alle anderen Achsen (Events, Metriken, Insights) und bewertet diese. Heimgeist erkennt z.B. Drift (Abweichungen von Normverhalten), Fehlstellen (Bereiche, in denen kaum Daten vorliegen) oder Prioritäten (wo das System sich mehr kümmern will). Rolle: Reflexionszentrum und Koordinator für Anpassungen.
*   **heimlern** unterstützt die Reflexion durch Mustererkennung über längere Zeiträume und Policy-Feedback. Es kann wiederkehrende Patterns aus Chronik+Insights extrahieren und dem System zurückspiegeln (z.B. „Fehler X tritt immer montags auf“). Rolle: lernende Adaption.
*   **hausKI** empfängt ggf. Meta-Feedback von heimgeist/heimlern und kann seine Entscheidungslogik anpassen (Orchestrierung neuer Policies) – relevant, wenn Reflexion in Aktionen mündet.
*   **metarepo** definiert einige Contracts für diese Ebene: z.B. policy.snapshot.schema.json (Abbild der aktuellen System-Policies) und policy.weight_adjustment.v1 (Schema für eine empfohlene Policy-Anpassung). Diese helfen, Reflexions-Ausgaben formal festzuhalten. Auch heimgeist.insight.v1 Schema (siehe Phase 4) fällt hier hinein, wenn heimgeist eigene Insight-Typen ausgibt.
*   **leitstand** aggregiert die Reflexionsdaten und präsentiert sie nutzerfreundlich.
*   **wgx** kommt indirekt vor: heimgeist könnte WGX anstoßen, zusätzliche Checks durchzuführen, wenn er Lücken bemerkt (z.B. „bitte führe intensiveren Test in Repo Y aus“). Soweit das sichtbar gemacht wird, könnte Leitstand solche Reflexions-Aktionen ebenfalls anzeigen.

**Artefakte und Datenflüsse:** Wichtigstes Artefakt sind Delta-Analysen und Empfehlungen. Beispiele: ein Drift-Report (Vergleich von Systemzuständen zwischen zwei Zeitpunkten), der z.B. sagt „die Eventrate von Typ Z ist um 30% gesunken gegenüber letzter Woche“. Ein weiteres Artefakt: Policy-Feedback – heimgeist/heimlern könnten Vorschläge generieren, etwa ein JSON policy.feedback mit Hinweis „Logge Event X häufiger, Datenlücke erkannt“ oder „Parameter P anpassen, Pattern erkannt“. Diese könnten in chronik als Events fließen oder als separate Dateien in einem Insights-Digest auftauchen. Datenflüsse: Heimgeist konsumiert kontinuierlich die Outputs aus Phase 2-4 (Events, Health, Insights). In regelmäßigen Abständen (z.B. täglich oder bei Triggern) erzeugt er Reflexions-Events oder Dateien. Ein typischer Fluss: Nach Erzeugung von insights.daily nimmt heimgeist diese plus den gestrigen Stand und berechnet Differenzen, erkennt vielleicht „neue Kategorie von Fehlern aufgetaucht“ und speichert das als Eintrag in einer Delta-Liste. Leitstand holt diese Delta-Daten ab (z.B. insights.digest als stark verdichtetes, für Menschen bestimmtes Resultat). Ggf. fließen bestimmte Reflexionsresultate zurück in den Systemkreislauf: etwa heimgeist triggert via plexer ein Event „Anomalie erkannt“ an hausKI. Visuell relevant sind primär die Outputs, die der Leitstand abgreift (Delta-Reports, Meta-Insights, Vorschläge).

**Verantwortlichkeiten pro Repo:** Erzeugung: heimgeist erstellt Meta-Insights und Empfehlungen. Es durchforstet die vorliegenden Daten nach höhergradigen Mustern – z.B. Trendbrüche, Korrelationen über mehrere Achsen (Event vs. Metric). heimlern liefert ihm dazu möglicherweise die statistische Signifikanz von Mustern oder direkt Änderungsimpulse (z.B. neu gelerntes Modell, das bessere Parameter vorschlägt). Validierung: Meta-Erkenntnisse sind schwer formal zu validieren – hier kommen Heuristiken zum Einsatz: heimgeist könnte jedem Befund einen Confidence-Wert geben. Dennoch könnten CI-Checks sicherstellen, dass heimgeist-Ausgaben dem Schema entsprechen (z.B. ein heimgeist_insight JSON ist gültig). Möglicherweise werden manche Empfehlungen manuell gegengeprüft bevor Umsetzung (z.B. ein Entwickler schaut auf vorgeschlagene Policy-Änderungen). Konsum: Leitstand zeigt diese Reflexionsresultate direkt an Menschen. hausKI konsumiert bestimmte Ausgaben, um das System autonom anzupassen (z.B. heimgeist schlägt vor, einen neuen Datenfeed einzubinden – hausKI könnte dies über Orchestrierung umsetzen). Auch metarepo kann indirekt Konsument sein: Wenn grundlegende Architektur-Änderungen empfohlen werden, fließen die Erkenntnisse dort in neue Richtlinien.

**Visualformen im Leitstand:** Hier steht die Delta-Ansicht im Vordergrund. Das kann mehrere Formen annehmen: Zum einen Trend-Charts mit Markierungen – z.B. Graphen, die den Verlauf einer Metrik zeigen, und heimgeist hebt den Punkt hervor, ab dem eine Drift beginnt (vielleicht mit Kommentar „Drift erkannt ab 5. Jan“). Zum anderen Vergleichsansichten: Man könnte zwei Zustände des Struktur-Graphen übereinanderlegen (z.B. heute vs. vor 1 Monat) und Änderungen hervorheben (neue Kanten in anderer Farbe, wegfallende blass) – visualisiert System-Evolution. Auch Listen von Erkenntnissen in Textform sind wichtig: z.B. „Offene Fragen:“ oder „Risiken:“ mit Stichpunkten. Diese stammen aus heimgeists Analyse, etwa „Repo X hat seit 7 Tagen keine Events – mögliches Instrumentierungsproblem“ oder „Neue Event-Art Y ohne zugehörige Policy – unbekanntes Muster“. Solche Punkte könnten im UI als Warnhinweise oder To-do-Liste erscheinen. Ebenfalls denkbar: Confidence-Meter – eine Anzeige, wie sicher das System sich seiner Daten ist (z.B. ein Balken je Achse: Datenabdeckung 80% – heimgeist schätzt ein, wie vollständig die Informationslage pro Achse ist). Die Visualisierung sollte vermitteln, wo das System sich unsicher ist. Insgesamt fungiert der Leitstand hier als Spiegel des Systembewusstseins: nicht nur hübsche Graphen, sondern auch Anzeigen wie „Hier fehlt mir Wissen“. Diese Phase gibt dem Benutzer einen Reflexionsraum, um gemeinsam mit dem System blinde Flecken zu adressieren.

**Schema- oder Contract-Typen:** Heimgeist-Insight Schema: Falls heimgeist eigene Insight-Objekte ausgibt, ist das im Schema heimgeist.insight.v1 festgehalten (z.B. Felder: finding_type = drift/lücke, description, confidence, related_data_refs etc.). Policy Snapshot (policy.snapshot.schema.json): dient dazu, den aktuellen Stand aller steuernden Parameter/Policies festzuhalten. Heimgeist könnte damit vor/nach Anpassungen arbeiten (Delta bilden). Policy Feedback/Adjustments (policy.weight_adjustment.v1 etc.): Schema für Vorschläge zur Policy-Anpassung – damit wird formatiert beschrieben, wie sich z.B. eine Gewichtung in hausKI ändern sollte. Insights Digest (insights.digest): ein stark verdichteter Output (eventuell in natürlicher Sprache oder Markdown) für menschliche Leser, generiert auf Basis der daily Insights. Dieses könnte als Artefakt insights.digest.md vorliegen und im Leitstand direkt angezeigt werden. Alle diese Schemas liegen ebenfalls im Metarepo bzw. werden dort versioniert, da es interne Organismus-Formate sind. Durch solche Contracts ist selbst die Meta-Reflexion standardisiert – das System spricht über sich selbst in einer definierten „Sprache“.

**Guards, Validierung und Update-Zyklen:** Guards: Es gibt möglicherweise Reflexions-Guards – z.B. ein Mechanismus, der sicherstellt, dass heimgeist sich nicht in Endlosschleifen verrennt (Meta-Überwachung der Überwachung). Beispielsweise könnte ein Guard prüfen, dass heimgeist-Ausgaben nicht explosive Größe erreichen (kein 100MB-Digest produzieren) oder dass ein empfohlenes Policy-Update nicht gegen grundlegende Policies verstößt. Validierung hier ist größtenteils inhaltlich/qualitativ: Man kann schwer vollautomatisch validieren, ob „Drift erkannt“ stimmt – dafür könnte man zweistufig vorgehen (heimgeist meldet, ein menschlicher Maintainer bestätigt). Update-Zyklus: Reflexion kann periodisch (z.B. wöchentlich ein großer Systemreport) und ereignisgetrieben erfolgen. Heimgeist wird ständig lauschen, aber nicht alles sofort melden – wahrscheinlich sammelt er Beobachtungen und reportet z.B. einmal täglich oder bei signifikanten Ereignissen. Ein sinnvolles Pattern: kontinuierliche Hintergrund-Überwachung mit Schwellen: kleine Abweichungen ignoriert er, größere triggert er sofort. In Leitstand sieht man also teils Ad-hoc-Warnungen (wenn kritische Lücke entdeckt) als auch regelmäßige Berichte. Die Integrität dieser Phase beruht darauf, dass das System ehrlich über seine Unsicherheiten berichtet – was wiederum in Code als feste Policies verankert ist (z.B. „report Uncertainty if confidence < X“).

**Ungewissheiten und epistemische Leerräume:** Paradoxerweise sind Ungewissheiten selbst Gegenstand dieser Phase – aber nicht alle können beseitigt werden. Heimgeist kann nur jene unbekannten Bereiche aufzeigen, die innerhalb seines Beobachtungshorizonts liegen. Unbekanntes unbekannt bleibt ein Problem: Es mag Faktoren geben, die dem gesamten Heimgewebe-Konzept fehlen. Diese Phase lässt Raum für das Eingeständnis: Das System weiß, dass es nicht alles weiß. Bewusst unmessbar bleibt z.B. „Wie nah ist das System an vollständigem Wissen?“ – es gibt keinen absoluten Maßstab dafür. Stattdessen wird versucht, relative Lücken zu identifizieren (z.B. „in Bereich X haben wir 0 Events, während in vergleichbaren Bereichen Y viele vorliegen“). Auch Qualitätsbewertungen der Erkenntnisse selbst bleiben eine Grauzone – heimgeist kann zwar Confidence-Werte geben, aber letztlich nicht garantieren, dass er nicht doch falsch liegt. Diese Meta-Unsicherheit (die Ungewissheit über die eigenen Schlussfolgerungen) ist etwas, was kein rein autonomes System vollends quantifizieren kann. Daher sind epistemische Leerräume hier inhärent: Das System belässt gewisse Dinge im Unbestimmten, um nicht Scheinsicherheit zu erzeugen. Beispielsweise könnte heimgeist bewusst keine Prozentzahl ala „98% aller Events erfasst“ ausgeben, da die letzte Meile nie sicher ist. Diese bewussten Lücken laden den menschlichen Operator dazu ein, kritisch mitzudenken. Epistemische Bescheidenheit ist Teil der Leitlinien: das Leitstand-Reflexionsmodul zeigt Unsicherheit eher an, als sie zu verstecken. Damit bleibt zwar eine Rest-Ungewissheit, aber genau die wird transparent gemacht – das ist die höchste Form der Reflexion im Heimgewebe.

## Gesamtbild

Diese fünf Phasen zusammen ergeben die Erkenntnisstruktur des Leitstandes als epistemischer Projektor. Jede Phase projiziert eine andere Schicht der Realität des Heimgewebes: von der physischen Struktur (Anatomie), über das funktionale Verhalten (Physiologie), die zeitliche Erfahrung (Chronik), die Bedeutungskonstitution (Erkenntnisschicht) bis hin zur Selbstwahrnehmung (Reflexion). Statt eines traditionellen UI-Designs haben wir hier eine wissenszentrierte Architektur: Der Leitstand dient nicht bloß der Anzeige, sondern der Sinnstiftung – er macht explizit, was sonst implizit bliebe. Durch klare Contracts und Verantwortlichkeiten in der Fleet (metarepo, wgx, chronik, semantAH, heimgeist, etc.) ist jedes Element dieser Erkenntnisstruktur rückverfolgbar und überprüfbar. Zugleich bleiben bewusst Freiräume für das, was das System (noch) nicht weiß. Diese Blaupause stellt sicher, dass das Heimgewebe sich selbst und seinen Betrachtern verständlich und ehrlich darbietet – als lebendiger, lernender Organismus aus Daten und Wissen.
